\chapter{Timing attacks}
	\index{Timing attacks}Come anticipato nel capitolo precedente, approfondiremo adesso la tipologia di attacchi basati sul tempo focalizzandoci maggiormente su quelli che hanno come obiettivo la cache del processore. 
	
	L'idea di base che sta sotto i timing attacks è quella che l'esecuzione di un determinato programma, al variare delle operazioni che vengono eseguite e al variare degli input, impiega tempi diversi per portare a termine il proprio compito.
	
	\lstinputlisting[language={Java},caption={Esempio di funzione vulnerabile ad un timing attack},label={list:timingBase}]{code/timingBase.txt}
		
	Ad esempio il \cref{list:timingBase} se fatto girare con la stringa ("passwordToBeStolen") impiegherà un tempo maggiore rispetto allo stesso programma fatto girare con la stringa ("foo"). Nel primo caso infatti verrà scansionata tutta la stringa mentre nel secondo caso si interromperà immediatamente. Questa informazione può essere utilizzata dall'attaccante per capire la stringa esatta. Per portare questo concetto al livello che ci interessa vediamo prima delle nozioni fondamentali sulla cache del processore. 
	
	\section{La cache del processore}
		Solitamente, ogni programma, tende a riutilizzare, nel tempo, un dato contenuto allo stesso indirizzo di memoria (località temporale\index{Località temporale}) e più dati localizzati in indirizzi vicini nella memoria (località spaziale\index{Località spaziale}). Ad esempio, se nel programma è presente un loop, lo stesso codice sarà eseguito più e più volte nel tempo verosimilmente con gli stessi dati.
		
		Dato che la differenza di velocità tra le memorie e la capacità di calcolo dei processori aumenta sempre di più \cite{hennessy2011computer}, la banda del bus di comunicazione e la velocità di accesso alla memoria principale sono diventati un fattore limitante sul throughput generale del processore. Per migliorare la località temporale e superare questo collo di bottiglia vengono utilizzate le caches\index{Cache}. 
		
		La cache è infatti un piccolo banco di memoria molto veloce sito all'interno di ogni core che il processore utilizza per immagazzinare i valori delle celle di memoria accedute più recentemente. 
		
		\subsection{Struttura della cache}
			I processori moderni hanno generalmente due livelli di cache per ogni core chiamati rispettivamente L1 e L2 ed un terzo livello chiamato \ac{LLC} o L3 condiviso tra i tutti i core (nella \cref{tab:processori} sono riportate le dimensioni delle varie memorie di alcuni processori). 
			
			\begin{table}[]
				\footnotesize
				\centering
				\begin{tabular}{|c|c|c|c|c|c|c|} \hline
					Casa		& Nome			& Core	& L1			& L2			& L3		& Prezzo (\$)	\\ \hline \hline
					Intel		& i3-530		& 2		& 2 x 64 KB		& 2 x 512KB		& 1 x 4 MB	& 113			\\ \hline
								& i5-6685R		& 4		& 4 x 64 KB		& 4 x 256KB		& 1 x 6 MB	& 288			\\ \hline
								& i7-6950X		& 10	& 10 x 64 KB	& 10 x 256KB	& 1 x 25 MB	& 1723			\\ \hline
								& i9-8950HK		& 6		& 6 x 64 KB		& 6 x 256KB		& 1 x 12 MB	& 583			\\ \hline
					AMD			& A4 Pro-3350B	& 4		& 4 x 64 KB		& 1 x 2MB		& -			& -				\\ \hline
								& Athlon 5370	& 4		& 4 x 64 KB		& 1 x 2MB		& -			& 55			\\ \hline
								& Epyc 7251		& 8		& 8 x 96 KB		& 8 x 512KB		& 32 MB		& 475			\\ \hline
				\end{tabular}
				\caption{Caratteristiche di alcuni processori}
				\label{tab:processori}
			\end{table}
			
			Considerando che l'accesso alla memoria principale in media impiega dai 50 ai 150 \emph{ns} mentre l'accesso alla cache L1 utilizza un tempo nell'ordine degli 0.3 \emph{ns} si può capire l'enorme differenza di prestazioni che possono essere raggiunte utilizzando questo tipo di memoria.
		
			Nella \cref{fig:cachei5} si può vedere l'architettura del processore quadcore Intel Core i5-3470. La gerarchia delle cache è organizzata in una memoria L1 di 64KB (divisa in 32KB per le istruzioni e 32KB per i dati), una memoria L2 da 256KB ed una memoria L3 da 6MB.
			
			\begin{figure}
				\begin{center}
					\includegraphics[scale=0.6]{cachei5}
					\caption{Architettura del processore Intel Core i5-3470}
					\label{fig:cachei5}
				\end{center}
			\end{figure}
			
			Andiamo ad analizzare più nel dettaglio le caratteristiche di una singola cache\cite{ge2016survey,yarom2014flush+}.
			
			\subsubsection{Cache lines}
				\index{Cache lines}Per sfruttare anche la località spaziale le caches sono divise in lines. Una cache line contiene un blocco di bytes adiacenti (generalmente di dimensione congrua ad una potenza di 2) caricati dalla memoria. Se uno qualunque dei bytes deve essere rimosso (si parla di \emph{evicting}\index{Evict}) per far spazio ad un altro dato, viene ricaricata l'intera line.
				
			\subsubsection{Associatività}
				Teoricamente una qualunque posizione di memoria può essere mappata in una qualunque cache line ed una cache ad \emph{n} lines potrebbe contenere \emph{n} linee qualunque dalla memoria. Questo tipo di cache viene chiamato \emph{fully-associative cache}\index{Fully-associative cache} ed è la migliore in teoria perché può sempre essere usata al massimo delle sue capacità e i cache miss si hanno solamente quando non c'è più spazio libero nella cache. In pratica però questo si traduce in un controllo in parallelo di tutte le linee che aumenta la complessità architetturale e il consumo di energia.
				
				L'estremo opposto è chiamato \emph{direct-mapped cache}\index{Direct-mapped cache}. In questo sistema ogni locazione di memoria può stare in una sola cache line, ben determinata da una funzione di indicizzazione. Due locazioni di memoria che mappano sulla stessa cache line non possono essere immagazzinate contemporaneamente e il loading di una comporta inevitabilmente l'evicting dell'altra. Questo potrebbe portare ad avere dei miss anche con la cache semivuota.
				
				Concretamente viene utilizzata una via di mezzo tra queste due soluzioni chiamata \emph{set-associative cache}\index{Set-associative cache}. La cache viene divisa in \emph{sets} (generalmente di dimensione compresa tra 2 e 24 lines) in cui ogni indirizzo viene controllato in parallelo come in una fully-associative cache. In quale set viene mappato un blocco di memoria viene calcolato come per una direct-mapped cache da una funzione del suo indirizzo. Una cache con \emph{n} line sets viene chiamata \emph{n-way associative} (\cref{fig:cachefill}).
				
				Si può notare che le direct-mapped e le fully-associative cache non sono altro che casi particolari di set-associative cache rispettivamente 1-way associative ed N-way associative (dove N è il numero di linee della cache).
				
				\begin{figure}
					\begin{center}
						\includegraphics[scale=.35]{cachefill}
						\caption{Schemi di associatività della cache.}
						\label{fig:cachefill}
					\end{center}
				\end{figure}
				
			\subsubsection{Inclusività}
				Una caratteristica che verrà sfruttata per montare l'attacco è l'\emph{inclusività}\index{Inclusività}. 
				
				Ogni livello superiore di cache contiene un sottoinsieme dei dati contenuti dal livello direttamente inferiore. Per mantenere questa caratteristica, quando viene eseguito un evicting di un dato da un livello inferiore, questo viene rimosso anche da tutti i livelli superiori.
				
	\section{Cache attacks}
		Per capire come funzionano la maggior parte degli attacchi alle cache prendiamo in considerazione un array di dati. Quando un elemento di questo array viene acceduto possono verificarsi una di queste due condizioni:
		
		\begin{enumerate}
			\item Il dato è presente in cache, si verifica una hit e viene recuperato molto velocemente.
			\item Il dato non è presente in cache, si verifica una miss e bisogna aspettare che venga recuperato dalla memoria principale.
		\end{enumerate}
		
		La differenza tra le due esecuzioni è notevole (diversi ordini di grandezza) ed è questa l'informazione utilizzata nell'attacco.
		
		\subsection{Tassonomia}
			Una prima classificazione dei cache attacks si basa sullo stato della cache al momento dell'attacco\cite{canteaut2006understanding}.
			
			\begin{itemize}
				\item \emph{Empty initial state}\index{Empty initial state} (reset attacks): questi attacchi si basano sull'assunzione che nessun dato che dovrà essere utilizzato dalla vittima è presente in cache.
				\item \emph{Forged initial state}\index{Forged initial state} (initialization attacks): in questo caso l'attaccante deve essere in grado di portare la cache in uno stato noto prima di poter effettuare l'attacco.
				\item \emph{Loaded initial state}\index{Loaded initial state} (micro-architecture attacks): la cache contiene tutti i dati necessari alla vittima per eseguire il programma.
			\end{itemize}
		
			Nello stesso lavoro si fornisce una classificazione anche in base al tipo di cache miss. 
			
			\begin{itemize}
				\item \emph{Cold start misses}\index{Cold start misses}: questo tipo di miss si ottiene quando il dato viene acceduto per la prima volta e quindi non è ancora mai stato caricato in cache.
				\item \emph{Capacity misses}\index{Capacity misses}: questo tipo di miss si ottiene quando si cerca di accedere a porzioni di memoria più grandi della dimensione della cache che quindi non possono essere presenti contemporaneamente.
				\item \emph{Conflict misses}\index{Conflict misses}: questo tipo di miss si ottiene quando un accesso precedente alla nostra richiesta ha provocato la eviction del dato di interesse (che era presente in cache).
			\end{itemize}
			
			In \cite{lipp2016armageddon,ge2016survey} si classificano gli attacchi in base all'approccio utilizzato:
			
			\begin{figure}
				\begin{center}
					\includegraphics[scale=0.4]{cacheattacks}
					\caption{Schema di attacco Prime+Probe (a) e Evict+Time(b)}
					\label{fig:cacheattacks}
				\end{center}
			\end{figure}
			
			\begin{itemize}
				\item \emph{Prime+Probe}\cite{osvik2006cache}\index{Prime+Probe}: Questo è un attacco di tipo forged initial state. L'attaccante precarica uno o più set della cache con dati propri. Dopo l'esecuzione della funzione vittima prova a riaccedere ad i propri dati. Se la funzione vittima non ha utilizzato lines mappate nei cache set occupati dall'attaccante, egli otterrà solo cache hit. Al contrario, se c'è stato l'evict di qualche line allora capirà quale ha utilizzato la vittima. Lo schema di questo attacco e del seguente è visibile in \cref{fig:cacheattacks}.
				\item \emph{Evict+Time}\cite{osvik2006cache}\index{Evict+Time}: Questo attacco è di tipo loaded initial state e suppone che tutti i dati che servono alla vittima siano già in cache. Questa condizione può essere ottenuta facendo eseguire una prima volta la funzione vittima. Con questa base, l'attaccante fa eseguire la funzione alla vittima calcolandone il tempo di esecuzione. Successivamente esegue una evict di un cache set caricando dati propri e fa eseguire nuovamente la funzione vittima. Se il tempo di questa ultima esecuzione è maggiore del precedente vuol dire che la funzione ha cercato di utilizzare il dato che è stato rimosso dalla cache ed ha dovuto aspettare di recuperarlo dalla memoria principale.
				\item \emph{Flush+Reload}\cite{yarom2014flush+}\index{Flush+Reload}: Questo attacco è una variante di Prime+Probe. L'attacco si divide in tre fasi. Nella prima fase l'attaccante esegue l'evict della linea a cui è interessato utilizzando l'istruzione \emph{clflush}\cite{intel64and}\index{Clflush} che invalida il dato su tutti i livelli della cache. Nella seconda fase aspetta che la vittima esegua la propria funzione. Nella terza fase l'attaccante ricarica la linea che aveva rimosso. Se la risposta è veloce vuol dire che la vittima l'ha portata in cache durante l'esecuzione della sua funzione.
				\item \emph{Evict+Reload}\cite{gruss2015cache}\index{Evict+Reload}: Una variante del Flush+Reload che utilizza la eviction al posto dell'istruzione di flush. Questa variante è poco utile se il processore sotto attacco è della famiglia x86 in quanto l'istruzione \emph{clflush} non richiede alcun privilegio mentre assume un certo rilievo se l'obiettivo è quello di attaccare un processore che non fornisce, nel suo set di istruzioni, una istruzione non privilegiata in grado di rimuovere dati dalla cache (come ad esempio quelli della famiglia ARM).
				\item \emph{Flush+Flush}\cite{gruss2016flush+}\index{Flush+Flush}: Diversamente da tutti i precedenti approcci, in questo caso non si esegue nessun accesso alla memoria ma l'attaccante si basa solamente sul tempo impiegato dall'istruzione \emph{clflush}. In \cite{lipp2016armageddon} si fa vedere come l'esecuzione di questa funzione abbia tempi differenti se chiamata su un indirizzo presente in cache o meno.  
			\end{itemize}
		
		\section{Contromisure possibili}
			Le difese da questo tipo di attacchi sono sia software che hardware e si dividono in 5 grandi famiglie\cite{ge2016survey}
			
			\begin{description}
				\item[Tecniche a tempo costante:] L'idea di base è quella di rendere il comportamento del codice che esegue operazioni critiche indipendente dai dati. Per esempio cercare di rendere una funzione crittografica indipendente sia dalla chiave che dall'input. Questo può essere ottenuto facendo eseguire istruzioni inutili per uniformare il tempo di esecuzione o accedendo a dati casuali dalla memoria per confondere l'attaccante sull'utilizzo della cache. 
				
				Queste soluzioni ovviamente portano ad una drastica perdita di prestazioni. Il tempo di esecuzione dovrà infatti tendere al tempo di esecuzione massimo ogni volta che sarà necessario richiamare la funzione.
				
				L'altro grande problema di questo tipo di soluzioni è quello della differenza di risultati su hardware differenti. Ad esempio \emph{Cock et al.}\cite{cock2014last} hanno dimostrato che la correzione a tempo costante adottata per mitigare l'attacco \emph{Lucky 13}\cite{al2013lucky} in OpenSSL 1.0.1e non risolve il problema se fatto girare su un processore ARM AM3358. 
				\item[Inserimento di rumore:] Questa famiglia di contromisure tende a rendere inutilizzabili le misure ottenute dall'attaccante inserendo in ogni evento osservabile da qualsiasi processo una quantità di rumore tale da renderne impossibile una qualunque analisi\cite{hu1992reducing}. Questa soluzione, in teoria, riesce a risolvere completamente il problema ma è stato dimostrato\cite{cock2014last} che in pratica non è applicabile. La quantità di rumore da produrre e da aggiungere alla computazione è talmente elevata che il sistema impiegherebbe la maggior parte delle sue risorse in questa operazione piuttosto che nella effettiva computazione del programma.
				\item[Imporre determinismo:] In questo caso si cerca di eliminare qualsiasi tipo di misura sul tempo eliminando completamente le variazioni di tempo visibile. Ad esempio in \cite{aviram2012efficient} si propone di eliminare completamente l'accesso al tempo reale fornendo all'esterno solamente un clock virtuale il cui avanzamento è completamente deterministico e indipendente dalle azioni di componenti vulnerabili. Per ottenere questo risultato si cerca di sincronizzare tutti i clock con l'esecuzione di un singolo processo, indipendente da input o azioni esterne, che esegue in tempo costante.
				\item[Suddividere il tempo:] In questo caso si cerca di suddividere il tempo in sezioni nelle quali si fornisce un accesso esclusivo all'hardware condiviso. Ci sono diverse tecniche per ottenere questo risultato uno dei quali è la cancellazione completa della cache ad ogni context switch(\emph{cache flushing}\index{Cache flushing}\cite{zhang2013duppel}). Questo ovviamente porta ad una perdita in prestazioni molto grande e si è passati al \emph{lattice scheduling}\index{Lattice scheduling}\cite{denning1976lattice} che esegue il flushing della cache non ad ogni context switch ma solo nel passaggio da processi sensibili a processi inaffidabili. Un'altra soluzione\cite{varadarajan2014scheduler} mira a sfruttare la necessità di analizzare molto spesso lo stato della cache della vittima che hanno attacchi di tipo Prime+Probe ad esempio. Questa necessità non viene alimentata imponendo un tempo minimo di esecuzione per le componenti vulnerabili entro il quale non possono essere prelazionate.
				\item[Suddividere le risorse hardware:] Attacchi eseguiti da processi concorrenti possono essere evitati solamente suddividendo adeguatamente le risorse hardware tra i vari processi. Per quanto riguarda la cache sono state avanzate varie proposte. Percival\cite{percival2005cache} suggerisce di suddividere la cache L1 tra i vari processi in modo tale da non permettere ad un processo di accedere o rimuovere lines utilizzate da un altro. Wang and Lee\cite{wang2007new} propongono invece la \emph{partition-locked cache}, un meccanismo hardware che permette di assegnare dei lock ad alcune lines contenenti dati particolarmente sensibili in maniera tale da non poter essere rimosse (ad esempio le tabelle di lookup di \ac{AES}).				 
			\end{description}